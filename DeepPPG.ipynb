{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepPPG.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRvy22LEJHjGGlqHlR2l9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/DeepPPG/blob/master/DeepPPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP9yrAFGaLro",
        "colab_type": "text"
      },
      "source": [
        "## Check if GPU is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8GFbCUSSJSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJTPX73j0cU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/analysiscenter/cardio.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqOoRMyl423u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cardio.batchflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nSTKTAk2bxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYNDcINgSfSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycHRcnLFgRuE",
        "colab_type": "code",
        "outputId": "f465f02a-bcb1-436e-81d4-259058514d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import random\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTKZtpAV9Q6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fRqm6asWvZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvOvs-tiXsHN",
        "colab_type": "text"
      },
      "source": [
        "## 1.Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcC3P6YHX2E7",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Import and Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exsQ-bulirlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Need to read data from different subjects (1-12 for training and validation, 13-15 for testing)\n",
        "def import_clean_data(directory):\n",
        "    ppg_S = []\n",
        "    acc_x_S = []\n",
        "    acc_y_S = []\n",
        "    acc_z_S = []\n",
        "    labels=[]\n",
        "\n",
        "    for subdir, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            with open(file, 'rb') as f:\n",
        "                data = pickle.load(f, encoding='bytes')          \n",
        "                #raw acc collected by 32Hz sampling rate\n",
        "                acc_raw = data[b'signal'][b'wrist'][b'ACC']\n",
        "                #raw ppg collected by 64Hz sampling rate\n",
        "                ppg_raw = data[b'signal'][b'wrist'][b'BVP']\n",
        "                #label for 8s data with 2s shift\n",
        "                label = data[b'label']\n",
        "\n",
        "                labels.append(label)\n",
        "\n",
        "                #convert shape from (len(data),1) to (len(data),None), so that it is compatible with signal.spectrogram\n",
        "                #ppg\n",
        "                ppg_raw_re = ppg_raw.reshape(len(ppg_raw))\n",
        "                #acc\n",
        "                acc_raw_x = []\n",
        "                acc_raw_y = []\n",
        "                acc_raw_z = []\n",
        "                for i in range(len(acc_raw)):\n",
        "                    acc_raw_x.append(acc_raw[i][0])\n",
        "                    acc_raw_y.append(acc_raw[i][1])\n",
        "                    acc_raw_z.append(acc_raw[i][2])\n",
        "                acc_x = np.array(acc_raw_x).reshape(len(acc_raw_x))\n",
        "                acc_y = np.array(acc_raw_y).reshape(len(acc_raw_y))\n",
        "                acc_z = np.array(acc_raw_z).reshape(len(acc_raw_z))\n",
        "\n",
        "                #Apply sliding-window (8s with 2s shift) to the dataset, with each segment of 1025 FFT points\n",
        "                #The result dimension for each channel is 51725 (HR points) x 257 (FFT points:frequency resolution)\n",
        "\n",
        "                #For ppg use nperseg = 64*8 = 512, overlap= 512 -128 = 384\n",
        "                ppg_freqs, ppg_times, ppg_Sx = signal.spectrogram(ppg_raw_re, fs=64, window='hanning',\n",
        "                                                      nperseg=512, noverlap=384,\n",
        "                                                      detrend=False, scaling='spectrum',nfft=4096)\n",
        "                #For acc use nperseg = 32*8 = 256, overlap = 256 - 64 = 192\n",
        "                acc_x_freqs, acc_x_times, acc_x_Sx = signal.spectrogram(acc_x, fs=32, window='hanning',\n",
        "                                                      nperseg=256, noverlap=192,\n",
        "                                                      detrend=False, scaling='spectrum',nfft=2048)\n",
        "                acc_y_freqs, acc_y_times, acc_y_Sx = signal.spectrogram(acc_y, fs=32, window='hanning',\n",
        "                                                      nperseg=256, noverlap=192,\n",
        "                                                      detrend=False, scaling='spectrum',nfft=2048)\n",
        "                acc_z_freqs, acc_z_times, acc_z_Sx = signal.spectrogram(acc_z, fs=32, window='hanning',\n",
        "                                                      nperseg=256, noverlap=192,\n",
        "                                                      detrend=False, scaling='spectrum',nfft=2048)\n",
        "\n",
        "                #Appy frequecy filtering (slice 0-4Hz, i.e.upto 240 bpm) and z-normalisation(zero mean and unit variance)\n",
        "                ppg_S.append(preprocessing.normalize(ppg_Sx[ppg_freqs <= 4]).T)\n",
        "                acc_x_S.append(preprocessing.normalize(acc_x_Sx[acc_x_freqs <= 4]).T)\n",
        "                acc_y_S.append(preprocessing.normalize(acc_y_Sx[acc_y_freqs <= 4]).T)\n",
        "                acc_z_S.append(preprocessing.normalize(acc_z_Sx[acc_z_freqs <= 4]).T)\n",
        "\n",
        "    #flat the list to np.array\n",
        "    ppg_Sx = np.asarray([item for sublist in ppg_S for item in sublist])\n",
        "    acc_x_Sx = np.asarray([item for sublist in acc_x_S for item in sublist])\n",
        "    acc_y_Sx = np.asarray([item for sublist in acc_y_S for item in sublist])\n",
        "    acc_z_Sx = np.asarray([item for sublist in acc_z_S for item in sublist])\n",
        "    label = np.asarray([item for sublist in labels for item in sublist])\n",
        "    \n",
        "    return ppg_Sx,acc_x_Sx,acc_y_Sx,acc_z_Sx,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiY1UrHgivCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('C:/Users/57lzhang.US04WW4008/Desktop/Deep Learning/PPG_FieldStudy/training')\n",
        "rootdir_training = os.getcwd()\n",
        "ppg_Sx_train,acc_x_Sx_train,acc_y_Sx_train,acc_z_Sx_train,label_train = import_clean_data(rootdir_training)\n",
        "\n",
        "os.chdir('C:/Users/57lzhang.US04WW4008/Desktop/Deep Learning/PPG_FieldStudy/testing')\n",
        "rootdir_testing = os.getcwd()\n",
        "ppg_Sx_test,acc_x_Sx_test,acc_y_Sx_test,acc_z_Sx_test,label_test = import_clean_data(rootdir_testing)\n",
        "\n",
        "#check dim, all of them should match\n",
        "print('ppg_Sx_train dim:',ppg_Sx_train.shape)   #(m,257)\n",
        "print('acc_x_Sx_train dim:',acc_x_Sx_train.shape)#(m,257)\n",
        "print('acc_y_Sx_train dim:',acc_y_Sx_train.shape)#(m,257)\n",
        "print('acc_z_Sx_train dim:',acc_z_Sx_train.shape)#(m,257)\n",
        "print(\"label_train dim:\",label_train.shape)     #(m,)\n",
        "\n",
        "#check dim, all of them should match\n",
        "print('ppg_Sx_test dim:',ppg_Sx_test.shape)   #(m,257)\n",
        "print('acc_x_Sx_test dim:',acc_x_Sx_test.shape)#(m,257)\n",
        "print('acc_y_Sx_test dim:',acc_y_Sx_test.shape)#(m,257)\n",
        "print('acc_z_Sx_test dim:',acc_z_Sx_test.shape)#(m,257)\n",
        "print(\"label_test dim:\",label_test.shape)     #(m,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3b5xRADYKPN",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Create training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmSzcS3NTn8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shuffle the data to break the time sequence\n",
        "def generate_train_val_data(source,label,train_ratio):\n",
        "    np.random.seed(1)\n",
        "    training_length = int(len(source) * train_ratio)\n",
        "    validation_length = int(len(source) - training_length)\n",
        "    \n",
        "    shuffled_set = []\n",
        "    shuffled_label_set =[]\n",
        "    randomRows = np.random.randint(len(source), size=len(source))\n",
        "    for i in randomRows:\n",
        "        shuffled_set.append(source[i])\n",
        "        shuffled_label_set.append(label[i])\n",
        "    train = shuffled_set[0:training_length]\n",
        "    train_label = shuffled_label_set[0:training_length]\n",
        "    validation = shuffled_set[:validation_length]\n",
        "    validation_label = shuffled_label_set[:validation_length]\n",
        "    return train, train_label, validation, validation_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7invpxsYRwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create training and validation, and testing sets\n",
        "#training/validation set\n",
        "tv_data_set = np.dstack((ppg_Sx_train,acc_x_Sx_train,acc_y_Sx_train,acc_z_Sx_train)) #convert 2D into 3D\n",
        "print('input training/validation data dimension is: ', tv_data_set.shape)\n",
        "\n",
        "#for data_set,split it into train and validation sets for evaluation purpose \n",
        "train,train_label,validation,validation_label = generate_train_val_data(tv_data_set,label_train,0.8)\n",
        "training_set = np.expand_dims(np.array(train),axis=1)\n",
        "validation_set = np.expand_dims(np.array(validation),axis=1)\n",
        "training_label_set = np.array(train_label)\n",
        "validation_label_set = np.array(validation_label)\n",
        "\n",
        "print(\"training_set dim:\",training_set.shape)\n",
        "print(\"training_label_set dim:\",training_label_set.shape)\n",
        "print(\"validation_set dim:\", validation_set.shape)\n",
        "print(\"validation_label_set dim:\", validation_label_set.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sp-RzXLYVT6",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Create testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbRCIFCXYYqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testing set\n",
        "testing_set = np.dstack((ppg_Sx_test,acc_x_Sx_test,acc_y_Sx_test,acc_z_Sx_test)) #convert 2D into 3D\n",
        "testing_set = np.expand_dims(testing_set,axis=1)\n",
        "testing_set_label = label_test\n",
        "print('input test data dimension is: ', testing_set.shape)\n",
        "print('input test data label dimension is:',testing_set_label.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY4ZWmhMYbC7",
        "colab_type": "text"
      },
      "source": [
        "##2.Tensorflow Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJOCjug5Yioa",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Build CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpbj1W7-YzKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorflow input shape = (m,1,257,4)\n",
        "# Better to experiment with different filter size and strides\n",
        "## Experiment with batch normalization to see if it improve accuracy\n",
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential([\n",
        "    #1st Conv2D\n",
        "    tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                           activation='relu', input_shape=(1,257,4)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #2nd Conv2D\n",
        "    tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #3rd Conv2D\n",
        "    tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #4th Conv2D\n",
        "    tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #5th Conv2D\n",
        "    tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    #Full connection layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvfEUDwaZJ9d",
        "colab_type": "text"
      },
      "source": [
        "Try learning rate schedule callbacks before increasing training epoches\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avjrb7DdY6ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator()\n",
        "validation_datagen = ImageDataGenerator()\n",
        "#train model\n",
        "\n",
        "#select the best learning rate\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-4 * 10**(epoch / 20))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mse', metrics=['mae'])\n",
        "` `\n",
        "#start training\n",
        "history = model.fit(train_datagen.flow(training_set,training_label_set,batch_size=32),\n",
        "                    steps_per_epoch=len(training_set)/32,\n",
        "                    epochs=50,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_datagen.flow(validation_set,validation_label_set,batch_size=32),\n",
        "                    validation_steps=len(validation_set)/32,\n",
        "                    callbacks=[lr_schedule]\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSabDDn8ZBX3",
        "colab_type": "text"
      },
      "source": [
        "Visualize learning rate vs epoches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXQ9cXp3ZIBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-4, 1e-1,0,1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1rh0UEOZMmY",
        "colab_type": "text"
      },
      "source": [
        "Try the optimized learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8XFIutuZO23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator()\n",
        "validation_datagen = ImageDataGenerator()\n",
        "model.compile(optimizer=Adam(lr=0.0005), loss='mse', metrics=['mae'])\n",
        "history = model.fit(train_datagen.flow(training_set,training_label_set,batch_size=32),\n",
        "                    steps_per_epoch=len(training_set)/32,\n",
        "                    epochs=200,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_datagen.flow(validation_set,validation_label_set,batch_size=32),\n",
        "                    validation_steps=len(validation_set)/32,\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apGf0bZAZRgH",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Evaluation of the training/validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp-eqha-ZXfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model error analysis\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Retrieve a list of list results on training and test data sets for each training epoch\n",
        "mae=history.history['mean_absolute_error']\n",
        "val_mae=history.history['val_mean_absolute_error']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(mae)) # Get number of epochs\n",
        "\n",
        "# Plot training and validation mae per epoch\n",
        "\n",
        "plt.plot(epochs, mae, 'r', label=\"Training MAE\")\n",
        "plt.plot(epochs, val_mae, 'b', label=\"Validation MAE\")\n",
        "plt.title('Training and validation MAE')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
        "plt.title('Training and validation Loss')\n",
        "plt.legend()\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMhxVNfxZcOy",
        "colab_type": "text"
      },
      "source": [
        "Save and load model if necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo13nBFIZah4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save('my_model1.h5')  # creates a HDF5 file 'my_model.h5' in working directory\n",
        "#del model  # deletes the existing model\n",
        "## returns a compiled model\n",
        "## identical to the previous one\n",
        "os.chdir('/users/57lzhang/Documents/data/PPG_FieldStudy')\n",
        "model = tf.keras.models.load_model('my_model_3_.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_-J2EGCZira",
        "colab_type": "text"
      },
      "source": [
        "# 3.Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qXxc6Y2ZoL6",
        "colab_type": "text"
      },
      "source": [
        "Compare the data predicted by \"Deep-PPG\" to the one generated by traditional DSP algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCy2uWdqZmQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error as mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nilxq-EqZsKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/users/57lzhang/Documents/data/PPG_FieldStudy')\n",
        "model = tf.keras.models.load_model('my_model_1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Z0PkMuZuzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mae_per_activity(directory):\n",
        "    i=0\n",
        "    for subdir,dirs,files in os.walk(directory):\n",
        "        if i<1:\n",
        "            i+=1\n",
        "            continue\n",
        "        else:\n",
        "            os.chdir(subdir)\n",
        "            ppg_Sx_test,acc_x_Sx_test,acc_y_Sx_test,acc_z_Sx_test,label_test = import_clean_data(subdir)\n",
        "            testing_set = np.dstack((ppg_Sx_test,acc_x_Sx_test,acc_y_Sx_test,acc_z_Sx_test)) #convert 2D into 3D\n",
        "            testing_set = np.expand_dims(testing_set,axis=1)\n",
        "            hr_estimate = model.predict(testing_set)\n",
        "            \n",
        "            #calculate overall MAE\n",
        "            score = model.evaluate(testing_set, label_test,batch_size=32,verbose=0)\n",
        "            print('Test subject:',files[0])\n",
        "            print('Overall MAE:', round(score[1],1))\n",
        "            \n",
        "            #plot the figure\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(range(len(hr_estimate)), hr_estimate, 'r', label=\"predicted HR\")\n",
        "            plt.plot(range(len(label_test)), label_test, 'b', label=\"actual HR\")\n",
        "            plt.legend()\n",
        "            plt.title(files[0])\n",
        "            plt.ylabel('Heart Rate(bpm)')\n",
        "            plt.xlabel('Time(2s)')\n",
        "            plt.axis([0,5000,40,250])\n",
        "            \n",
        "            #calculate MAE per activity\n",
        "            with open(files[0],'rb') as f:\n",
        "                data = pickle.load(f, encoding='bytes')          \n",
        "                #extract the raw 4Hz data\n",
        "                activity_raw = data[b'activity']            \n",
        "                time = pd.timedelta_range(0, periods=len(activity_raw), freq='0.25S')     \n",
        "                df = pd.DataFrame(activity_raw,index = time,columns=list('A'))\n",
        "                activity = df.resample('2S').max()\n",
        "                activity=activity[2:-1]\n",
        "                #Append True and Estimated HR\n",
        "                activity['HR_True'] = label_test\n",
        "                activity['HR_estimate'] = hr_estimate\n",
        "                activity.groupby('A').nunique()\n",
        "\n",
        "                print('Sitting:',round(mae(activity[activity['A']==1]['HR_True'],activity[activity['A']==1]['HR_estimate']),1))\n",
        "                print('Climbing:',round(mae(activity[activity['A']==2]['HR_True'],activity[activity['A']==2]['HR_estimate']),1))\n",
        "                print('Table Soccer:',round(mae(activity[activity['A']==3]['HR_True'],activity[activity['A']==3]['HR_estimate']),1))\n",
        "                print('Cycling:',round(mae(activity[activity['A']==4]['HR_True'],activity[activity['A']==4]['HR_estimate']),1))\n",
        "                print('Driving:',round(mae(activity[activity['A']==5]['HR_True'],activity[activity['A']==5]['HR_estimate']),1))\n",
        "                print('Lunch:',round(mae(activity[activity['A']==6]['HR_True'],activity[activity['A']==6]['HR_estimate']),1))\n",
        "                print('Walking:',round(mae(activity[activity['A']==7]['HR_True'],activity[activity['A']==7]['HR_estimate']),1))\n",
        "                print('Working:',round(mae(activity[activity['A']==8]['HR_True'],activity[activity['A']==8]['HR_estimate']),1))\n",
        "                print('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rX1a2uHZyY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/users/57lzhang/Documents/data/PPG_FieldStudy/test')\n",
        "rootdir_testing = os.getcwd()\n",
        "mae_per_activity(rootdir_testing)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}