{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepPPG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkDQ0CE4l0TFu+ZH3zb+rZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/DeepPPG/blob/master/DeepPPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP9yrAFGaLro",
        "colab_type": "text"
      },
      "source": [
        "## Check if GPU is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nSTKTAk2bxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYNDcINgSfSW",
        "colab_type": "code",
        "outputId": "7249023a-3bc8-4948-fdfc-464791337fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/63/13/ea9ff554aa0043540a2387c28dd7926575eb25cf89e598caecea836d189d/tensorflow_gpu-2.0.0-cp37-cp37m-win_amd64.whl\n",
            "Collecting absl-py>=0.7.0 (from tensorflow-gpu==2.0.0)\n",
            "Collecting keras-preprocessing>=1.0.5 (from tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
            "Collecting protobuf>=3.6.1 (from tensorflow-gpu==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/92/30/1b7ccde09bf0c535d11f18a574ed7d7572c729a8f754fd568b297be08b61/protobuf-3.11.3-cp37-cp37m-win_amd64.whl (1.0MB)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Collecting keras-applications>=1.0.8 (from tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
            "Collecting astor>=0.6.0 (from tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
            "Collecting google-pasta>=0.1.6 (from tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl\n",
            "Collecting gast==0.2.2 (from tensorflow-gpu==2.0.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl\n",
            "Collecting termcolor>=1.1.0 (from tensorflow-gpu==2.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Collecting grpcio>=1.8.6 (from tensorflow-gpu==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/6d/99aba8db04bf58193ed157dfe7e848494b93dd8aa3f6a4d1edfef318779c/grpcio-1.27.2-cp37-cp37m-win_amd64.whl (1.9MB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow-gpu==2.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (41.4.0)\n",
            "Requirement already satisfied: h5py in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.9.0)\n",
            "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/8d/e2ebbd0502627ed0d8a408162020e1c0792f088b49fddeedaaeebc206ed7/google_auth-1.11.2-py2.py3-none-any.whl (76kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.22.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/c4/ba46d44855e6eb1770a12edace5a165a0c6de13349f592b9036257f3c3d3/Markdown-3.2.1-py2.py3-none-any.whl (88kB)\n",
            "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
            "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\57lzhang.us04ww4008\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.9.11)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
            "Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: absl-py, keras-preprocessing, protobuf, keras-applications, astor, google-pasta, gast, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, grpcio, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, tensorflow-estimator, termcolor, opt-einsum, tensorflow-gpu\n",
            "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.0.0 gast-0.2.2 google-auth-1.11.2 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.27.2 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 oauthlib-3.1.0 opt-einsum-3.1.0 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0 termcolor-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycHRcnLFgRuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import random\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTKZtpAV9Q6H",
        "colab_type": "code",
        "outputId": "f20610e9-c101-499b-b870-b9b620e7957d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fRqm6asWvZe",
        "colab_type": "code",
        "outputId": "d114e476-d4b6-4f9f-a1d3-636620856328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 32335559164142901\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 6601766338\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 15959517662705351281\n",
            "physical_device_desc: \"device: 0, name: Quadro RTX 4000, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
            ", name: \"/device:GPU:1\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 6601766338\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 7960549734677816107\n",
            "physical_device_desc: \"device: 1, name: Quadro RTX 4000, pci bus id: 0000:05:00.0, compute capability: 7.5\"\n",
            ", name: \"/device:GPU:2\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 6601766338\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 9228658607591131853\n",
            "physical_device_desc: \"device: 2, name: Quadro RTX 4000, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
            ", name: \"/device:GPU:3\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 6601766338\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 5090589327258640047\n",
            "physical_device_desc: \"device: 3, name: Quadro RTX 4000, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftJ6y68Dy99A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvOvs-tiXsHN",
        "colab_type": "text"
      },
      "source": [
        "## 1.Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcC3P6YHX2E7",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Import and Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exsQ-bulirlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Need to read data from different subjects (1-12 for training and validation, 13-15 for testing)\n",
        "def import_clean_data(directory):\n",
        "    ppg_S = []\n",
        "    acc_x_S = []\n",
        "    acc_y_S = []\n",
        "    acc_z_S = []\n",
        "    labels=[]\n",
        "\n",
        "    for subdir, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            with open(file, 'rb') as f:\n",
        "                data = pickle.load(f, encoding='bytes')          \n",
        "                #raw acc collected by 32Hz sampling rate\n",
        "                acc_raw = data[b'signal'][b'wrist'][b'ACC']\n",
        "                #raw ppg collected by 64Hz sampling rate\n",
        "                ppg_raw = data[b'signal'][b'wrist'][b'BVP']\n",
        "                #label for 8s data with 2s shift\n",
        "                label = data[b'label']\n",
        "\n",
        "                labels.append(label)\n",
        "\n",
        "                #convert shape from (len(data),1) to (len(data),None), so that it is compatible with signal.spectrogram\n",
        "                #ppg\n",
        "                ppg_raw_re = ppg_raw.reshape(len(ppg_raw))\n",
        "                #acc\n",
        "                acc_raw_x = []\n",
        "                acc_raw_y = []\n",
        "                acc_raw_z = []\n",
        "                for i in range(len(acc_raw)):\n",
        "                    acc_raw_x.append(acc_raw[i][0])\n",
        "                    acc_raw_y.append(acc_raw[i][1])\n",
        "                    acc_raw_z.append(acc_raw[i][2])\n",
        "                acc_x = np.array(acc_raw_x).reshape(len(acc_raw_x))\n",
        "                acc_y = np.array(acc_raw_y).reshape(len(acc_raw_y))\n",
        "                acc_z = np.array(acc_raw_z).reshape(len(acc_raw_z))\n",
        "\n",
        "                #Apply sliding-window (8s with 2s shift) to the dataset, with each segment of 1025 FFT points\n",
        "                #The result dimension for each channel is 51725 (HR points) x 257 (FFT points:frequency resolution)\n",
        "\n",
        "                #For ppg use nperseg = 64*8 = 512, overlap= 512 -128 = 384\n",
        "                ppg_freqs, ppg_times, ppg_Sx = signal.spectrogram(ppg_raw_re, fs=64, window='hanning',\n",
        "                                                      nperseg=512, noverlap=384,\n",
        "                                                      detrend=False, scaling='spectrum',nfft=4096)\n",
        "                #For acc use nperseg = 32*8 = 256, overlap = 256 - 64 = 192\n",
        "                acc_x_freqs, acc_x_times, acc_x_Sx = signal.spectrogram(acc_x, fs=32, window='hanning',\n",
        "                                                      nperseg=256, noverlap=192,\n",
        "                                                      detrend=False, scaling='spectrum',nfft=2048)\n",
        "                acc_y_freqs, acc_y_times, acc_y_Sx = signal.spectrogram(acc_y, fs=32, window='hanning',\n",
        "                                                      nperseg=256, noverlap=192,\n",
        "                                                      detrend=False, scaling='spectrum',nfft=2048)\n",
        "                acc_z_freqs, acc_z_times, acc_z_Sx = signal.spectrogram(acc_z, fs=32, window='hanning',\n",
        "                                                      nperseg=256, noverlap=192,\n",
        "                                                      detrend=False, scaling='spectrum',nfft=2048)\n",
        "\n",
        "                #Appy frequecy filtering (slice 0-4Hz, i.e.upto 240 bpm) and z-normalisation(zero mean and unit variance)\n",
        "                ppg_S.append(preprocessing.normalize(ppg_Sx[ppg_freqs <= 4]).T)\n",
        "                acc_x_S.append(preprocessing.normalize(acc_x_Sx[acc_x_freqs <= 4]).T)\n",
        "                acc_y_S.append(preprocessing.normalize(acc_y_Sx[acc_y_freqs <= 4]).T)\n",
        "                acc_z_S.append(preprocessing.normalize(acc_z_Sx[acc_z_freqs <= 4]).T)\n",
        "\n",
        "    #flat the list to np.array\n",
        "    ppg_Sx = np.asarray([item for sublist in ppg_S for item in sublist])\n",
        "    acc_x_Sx = np.asarray([item for sublist in acc_x_S for item in sublist])\n",
        "    acc_y_Sx = np.asarray([item for sublist in acc_y_S for item in sublist])\n",
        "    acc_z_Sx = np.asarray([item for sublist in acc_z_S for item in sublist])\n",
        "    label = np.asarray([item for sublist in labels for item in sublist])\n",
        "    \n",
        "    return ppg_Sx,acc_x_Sx,acc_y_Sx,acc_z_Sx,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiY1UrHgivCE",
        "colab_type": "code",
        "outputId": "2bbef9d1-9977-4957-8669-c3fae56adfb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "os.chdir('C:/users/57lzhang.US04WW4008/Desktop/Deep Learning/PPG_FieldStudy/training')\n",
        "rootdir_training = os.getcwd()\n",
        "ppg_Sx_train,acc_x_Sx_train,acc_y_Sx_train,acc_z_Sx_train,label_train = import_clean_data(rootdir_training)\n",
        "\n",
        "os.chdir('C:/users/57lzhang.US04WW4008/Desktop/Deep Learning/PPG_FieldStudy/testing')\n",
        "rootdir_testing = os.getcwd()\n",
        "ppg_Sx_test,acc_x_Sx_test,acc_y_Sx_test,acc_z_Sx_test,label_test = import_clean_data(rootdir_testing)\n",
        "\n",
        "#check dim, all of them should match\n",
        "print('ppg_Sx_train dim:',ppg_Sx_train.shape)   #(m,257)\n",
        "print('acc_x_Sx_train dim:',acc_x_Sx_train.shape)#(m,257)\n",
        "print('acc_y_Sx_train dim:',acc_y_Sx_train.shape)#(m,257)\n",
        "print('acc_z_Sx_train dim:',acc_z_Sx_train.shape)#(m,257)\n",
        "print(\"label_train dim:\",label_train.shape)     #(m,)\n",
        "\n",
        "#check dim, all of them should match\n",
        "print('ppg_Sx_test dim:',ppg_Sx_test.shape)   #(m,257)\n",
        "print('acc_x_Sx_test dim:',acc_x_Sx_test.shape)#(m,257)\n",
        "print('acc_y_Sx_test dim:',acc_y_Sx_test.shape)#(m,257)\n",
        "print('acc_z_Sx_test dim:',acc_z_Sx_test.shape)#(m,257)\n",
        "print(\"label_test dim:\",label_test.shape)     #(m,)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ppg_Sx_train dim: (60221, 257)\n",
            "acc_x_Sx_train dim: (60221, 257)\n",
            "acc_y_Sx_train dim: (60221, 257)\n",
            "acc_z_Sx_train dim: (60221, 257)\n",
            "label_train dim: (60221,)\n",
            "ppg_Sx_test dim: (4476, 257)\n",
            "acc_x_Sx_test dim: (4476, 257)\n",
            "acc_y_Sx_test dim: (4476, 257)\n",
            "acc_z_Sx_test dim: (4476, 257)\n",
            "label_test dim: (4476,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3b5xRADYKPN",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Create training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmSzcS3NTn8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shuffle the data to break the time sequence\n",
        "def generate_train_val_data(source,label,train_ratio):\n",
        "    np.random.seed(1)\n",
        "    training_length = int(len(source) * train_ratio)\n",
        "    validation_length = int(len(source) - training_length)\n",
        "    \n",
        "    shuffled_set = []\n",
        "    shuffled_label_set =[]\n",
        "    randomRows = np.random.randint(len(source), size=len(source))\n",
        "    for i in randomRows:\n",
        "        shuffled_set.append(source[i])\n",
        "        shuffled_label_set.append(label[i])\n",
        "    train = shuffled_set[0:training_length]\n",
        "    train_label = shuffled_label_set[0:training_length]\n",
        "    validation = shuffled_set[:validation_length]\n",
        "    validation_label = shuffled_label_set[:validation_length]\n",
        "    return train, train_label, validation, validation_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7invpxsYRwM",
        "colab_type": "code",
        "outputId": "08ed2e9b-2d98-4bf6-cc2a-f419254e9ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#create training and validation, and testing sets\n",
        "#training/validation set\n",
        "tv_data_set = np.dstack((ppg_Sx_train,acc_x_Sx_train,acc_y_Sx_train,acc_z_Sx_train)) #convert 2D into 3D\n",
        "print('input training/validation data dimension is: ', tv_data_set.shape)\n",
        "\n",
        "#for data_set,split it into train and validation sets for evaluation purpose \n",
        "train,train_label,validation,validation_label = generate_train_val_data(tv_data_set,label_train,0.8)\n",
        "training_set = np.expand_dims(np.array(train),axis=1)\n",
        "validation_set = np.expand_dims(np.array(validation),axis=1)\n",
        "training_label_set = np.array(train_label)\n",
        "validation_label_set = np.array(validation_label)\n",
        "\n",
        "print(\"training_set dim:\",training_set.shape)\n",
        "print(\"training_label_set dim:\",training_label_set.shape)\n",
        "print(\"validation_set dim:\", validation_set.shape)\n",
        "print(\"validation_label_set dim:\", validation_label_set.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input training/validation data dimension is:  (60221, 257, 4)\n",
            "training_set dim: (48176, 1, 257, 4)\n",
            "training_label_set dim: (48176,)\n",
            "validation_set dim: (12045, 1, 257, 4)\n",
            "validation_label_set dim: (12045,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sp-RzXLYVT6",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Create testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbRCIFCXYYqa",
        "colab_type": "code",
        "outputId": "fc37c360-d184-4d30-cff0-adacae01307a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#testing set\n",
        "testing_set = np.dstack((ppg_Sx_test,acc_x_Sx_test,acc_y_Sx_test,acc_z_Sx_test)) #convert 2D into 3D\n",
        "testing_set = np.expand_dims(testing_set,axis=1)\n",
        "testing_set_label = label_test\n",
        "print('input test data dimension is: ', testing_set.shape)\n",
        "print('input test data label dimension is:',testing_set_label.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input test data dimension is:  (4476, 1, 257, 4)\n",
            "input test data label dimension is: (4476,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY4ZWmhMYbC7",
        "colab_type": "text"
      },
      "source": [
        "##2.Tensorflow Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJOCjug5Yioa",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Build the model and visualize it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpbj1W7-YzKL",
        "colab_type": "code",
        "outputId": "4b4859f7-e0a8-4dc5-b884-9428b47ce0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# tensorflow input shape = (m,1,257,4)\n",
        "# Better to experiment with different filter size and strides\n",
        "## Experiment with batch normalization to see if it improve accuracy\n",
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential([\n",
        "    #1st Conv2D\n",
        "    tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                           activation='relu', input_shape=(1,257,4)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #2nd Conv2D\n",
        "    tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #3rd Conv2D\n",
        "    tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #4th Conv2D\n",
        "    tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #5th Conv2D\n",
        "    tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    #Full connection layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 1, 257, 8)         40        \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1, 257, 8)         32        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 1, 128, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1, 128, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1, 126, 16)        400       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 126, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 63, 16)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 63, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 61, 32)         1568      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 61, 32)         128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 30, 32)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1, 30, 32)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 1, 28, 64)         6208      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1, 28, 64)         256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 1, 14, 64)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1, 14, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 1, 14, 16)         1040      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1, 14, 16)         64        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 224)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                14400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 24,521\n",
            "Trainable params: 24,121\n",
            "Non-trainable params: 400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvfEUDwaZJ9d",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Find the optimal learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avjrb7DdY6ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorflow input shape = (m,1,257,4)\n",
        "# Better to experiment with different filter size and strides\n",
        "## Experiment with batch normalization to see if it improve accuracy\n",
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#train model\n",
        "learning_rate = 0.0001\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      #1st Conv2D\n",
        "      tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                            activation='relu', input_shape=(1,257,4)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #2nd Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #3rd Conv2D\n",
        "      tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #4th Conv2D\n",
        "      tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #5th Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      #Full connection layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='mse', metrics=['mae'])\n",
        "\n",
        "#schedule a learning rate incline iteration\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-4 * 10**(epoch / 20))\n",
        "\n",
        "#start training\n",
        "train_datagen = ImageDataGenerator()\n",
        "validation_datagen = ImageDataGenerator()\n",
        "history = model.fit(train_datagen.flow(training_set,training_label_set,batch_size=32),\n",
        "                    steps_per_epoch=len(training_set)/32,\n",
        "                    epochs=50,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_datagen.flow(validation_set,validation_label_set,batch_size=32),\n",
        "                    validation_steps=len(validation_set)/32,\n",
        "                    callbacks=[lr_schedule]\n",
        "                  )\n",
        "\n",
        "#Visualize learning rate vs epoches\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-4, 1e-1,0,1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1rh0UEOZMmY",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Try the optimized learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8XFIutuZO23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0005 ##this value is learned from previous training\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      #1st Conv2D\n",
        "      tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                            activation='relu', input_shape=(1,257,4)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #2nd Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #3rd Conv2D\n",
        "      tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #4th Conv2D\n",
        "      tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #5th Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      #Full connection layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='mse', metrics=['mae'])\n",
        "\n",
        "#starting training\n",
        "train_datagen = ImageDataGenerator()\n",
        "validation_datagen = ImageDataGenerator()\n",
        "history = model.fit(train_datagen.flow(training_set,training_label_set,batch_size=32),\n",
        "                    steps_per_epoch=len(training_set)/32,\n",
        "                    epochs=200,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_datagen.flow(validation_set,validation_label_set,batch_size=32),\n",
        "                    validation_steps=len(validation_set)/32,\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apGf0bZAZRgH",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Evaluation of the training/validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp-eqha-ZXfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model error analysis\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Retrieve a list of list results on training and test data sets for each training epoch\n",
        "mae=history.history['mean_absolute_error']\n",
        "val_mae=history.history['val_mean_absolute_error']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(mae)) # Get number of epochs\n",
        "\n",
        "# Plot training and validation mae per epoch\n",
        "\n",
        "plt.plot(epochs, mae, 'r', label=\"Training MAE\")\n",
        "plt.plot(epochs, val_mae, 'b', label=\"Validation MAE\")\n",
        "plt.title('Training and validation MAE')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
        "plt.title('Training and validation Loss')\n",
        "plt.legend()\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMhxVNfxZcOy",
        "colab_type": "text"
      },
      "source": [
        "Save and load model if necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo13nBFIZah4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save('my_model1.h5')  # creates a HDF5 file 'my_model.h5' in working directory\n",
        "#del model  # deletes the existing model\n",
        "## returns a compiled model\n",
        "## identical to the previous one\n",
        "os.chdir('/users/57lzhang/Documents/data/PPG_FieldStudy')\n",
        "model = tf.keras.models.load_model('my_model_3_.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_-J2EGCZira",
        "colab_type": "text"
      },
      "source": [
        "# 3.Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qXxc6Y2ZoL6",
        "colab_type": "text"
      },
      "source": [
        "Compare the data predicted by \"Deep-PPG\" to the one generated by traditional DSP algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCy2uWdqZmQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error as mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nilxq-EqZsKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/users/57lzhang/Documents/data/PPG_FieldStudy')\n",
        "model = tf.keras.models.load_model('my_model_1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Z0PkMuZuzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mae_per_activity(directory):\n",
        "    i=0\n",
        "    for subdir,dirs,files in os.walk(directory):\n",
        "        if i<1:  #what is this??\n",
        "            i+=1\n",
        "            continue\n",
        "        else:\n",
        "            os.chdir(subdir)\n",
        "            ppg_Sx_test,acc_x_Sx_test,acc_y_Sx_test,acc_z_Sx_test,label_test = import_clean_data(subdir)\n",
        "            testing_set = np.dstack((ppg_Sx_test,acc_x_Sx_test,acc_y_Sx_test,acc_z_Sx_test)) #convert 2D into 3D\n",
        "            testing_set = np.expand_dims(testing_set,axis=1)\n",
        "            hr_estimate = model.predict(testing_set)\n",
        "            \n",
        "            #calculate overall MAE\n",
        "            score = model.evaluate(testing_set, label_test,batch_size=32,verbose=0)\n",
        "            print('Test subject:',files[0])\n",
        "            print('Overall MAE:', round(score[1],1))\n",
        "            \n",
        "            #plot the figure\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(range(len(hr_estimate)), hr_estimate, 'r', label=\"predicted HR\")\n",
        "            plt.plot(range(len(label_test)), label_test, 'b', label=\"actual HR\")\n",
        "            plt.legend()\n",
        "            plt.title(files[0])\n",
        "            plt.ylabel('Heart Rate(bpm)')\n",
        "            plt.xlabel('Time(2s)')\n",
        "            plt.axis([0,5000,40,250])\n",
        "            \n",
        "            #calculate MAE per activity\n",
        "            with open(files[0],'rb') as f:\n",
        "                data = pickle.load(f, encoding='bytes')          \n",
        "                #extract the raw 4Hz data\n",
        "                activity_raw = data[b'activity']            \n",
        "                time = pd.timedelta_range(0, periods=len(activity_raw), freq='0.25S')     \n",
        "                df = pd.DataFrame(activity_raw,index = time,columns=list('A'))\n",
        "                activity = df.resample('2S').max()\n",
        "                activity=activity[2:-1]\n",
        "                #Append True and Estimated HR\n",
        "                activity['HR_True'] = label_test\n",
        "                activity['HR_estimate'] = hr_estimate\n",
        "                activity.groupby('A').nunique()\n",
        "\n",
        "                print('Sitting:',round(mae(activity[activity['A']==1]['HR_True'],activity[activity['A']==1]['HR_estimate']),1))\n",
        "                print('Climbing:',round(mae(activity[activity['A']==2]['HR_True'],activity[activity['A']==2]['HR_estimate']),1))\n",
        "                print('Table Soccer:',round(mae(activity[activity['A']==3]['HR_True'],activity[activity['A']==3]['HR_estimate']),1))\n",
        "                print('Cycling:',round(mae(activity[activity['A']==4]['HR_True'],activity[activity['A']==4]['HR_estimate']),1))\n",
        "                print('Driving:',round(mae(activity[activity['A']==5]['HR_True'],activity[activity['A']==5]['HR_estimate']),1))\n",
        "                print('Lunch:',round(mae(activity[activity['A']==6]['HR_True'],activity[activity['A']==6]['HR_estimate']),1))\n",
        "                print('Walking:',round(mae(activity[activity['A']==7]['HR_True'],activity[activity['A']==7]['HR_estimate']),1))\n",
        "                print('Working:',round(mae(activity[activity['A']==8]['HR_True'],activity[activity['A']==8]['HR_estimate']),1))\n",
        "                print('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rX1a2uHZyY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/users/57lzhang/Documents/data/PPG_FieldStudy/test')\n",
        "rootdir_testing = os.getcwd()\n",
        "mae_per_activity(rootdir_testing)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}